---
title: "R Notebook"
output: html_notebook
---

```{r setup}
library(rvest)
library(stringi)
library(tidyverse)
library(V8)
```



Below code is based on the procedure here: https://gist.github.com/flovv/91453712e8a6ba957e63#file-scrape_final-js 

The starting URL is the first page of search results, which must be manually grabbed from a browser. That is, submit the search form in a browser manually, going through the list of election-related codes. Set the correct dates and other factors. Take the resulting url and enter it below. The rest should be automated.

First page of results:

```{r}
url <- paste0("https://sudact.ru/regular/doc/?page=1&regular-court=&regular-date_from=01.01.2000&regular-case_doc=&regular-lawchunkinfo=%D0%A1%D1%82%D0%B0%D1%82%D1%8C%D1%8F+142.1.+%D0%A4%D0%B0%D0%BB%D1%8C%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F+%D0%B8%D1%82%D0%BE%D0%B3%D0%BE%D0%B2+%D0%B3%D0%BE%D0%BB%D0%BE%D1%81%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F%28%D0%A3%D0%9A+%D0%A0%D0%A4%29&regular-workflow_stage=10&regular-date_to=&regular-area=&regular-txt=&_=1596897091676&regular-judge=")
  lines <- readLines("scrape_final.js")
  lines[1] <- paste0("var url ='", url ,"';")
  lines[12] <- "fs.write('1.html', page.content, 'w');"

  writeLines(lines, "scrape_final.js")
system("phantomjs scrape_final.js")


results <- read_html("1.html", encoding = "utf-8")

#other.pages.1.urls <- results %>% html_nodes("ul.pager") %>% html_nodes("li") %>% html_nodes("a") %>% html_attr("href")

#length.pager <- length(other.pages.1.urls)
#url2 <- paste0("https://sudact.ru/regular/doc/", other.pages.1.urls[length.pager])

#lines <- readLines("scrape_final.js")
#lines[1] <- paste0("var url ='", url2 ,"';")
#lines[12] <- "fs.write('pager.html', page.content, 'w');"
#writeLines(lines, "scrape_final.js")
#system("phantomjs scrape_final.js")
#results.pager <- read_html("pager.html", encoding = "utf-8")

#other.pages.2.urls <- results.pager %>% html_nodes("ul.pager") %>% html_nodes("li") %>% html_nodes("a") %>% html_attr("href") #This process goes to the farthest page in the javascript, step by step. I'm not sure if there's a better way to do this automatically, since html_session can't navigate the js. Best solution may be to manually get the relevant starting url, as well as the relevant max page.


```

Subsequent pages of results:

```{r}
for(j in 1:12){  #Set this manually, as one less than the max page number
url <- paste0("https://sudact.ru/regular/doc/", "?page=", j+1, "&regular-court=&regular-date_from=01.01.2000&regular-case_doc=&regular-lawchunkinfo=Статья+142.1.+Фальсификация+итогов+голосования%28УК+РФ%29&regular-workflow_stage=10&regular-date_to=&regular-area=&regular-txt=&_=1596904945233&regular-judge=") #Set this URL manually as well
  lines <- readLines("scrape_final.js")
  lines[1] <- paste0("var url ='", url ,"';")
  lines[12] <- paste0("fs.write('", j+1, ".html', page.content, 'w');")
  writeLines(lines, "scrape_final2.js")
system("phantomjs scrape_final2.js")
Sys.sleep(5)
}


```

First page decisions
```{r}
case.data <- matrix(NA, ncol = 2, nrow = 1)
case.data.temp <- case.data

for(j in 1:13){ #Update this number manually with max for each search
html.file <- paste0(j, ".html")
results <- read_html(html.file, encoding = "utf-8")

urls <- results  %>% html_nodes("ul.results") %>% html_nodes("li") %>% html_nodes("h4") %>% html_nodes("a") %>% html_attr("href")
case.info <- results  %>% html_nodes("ul.results") %>% html_nodes("li") %>% html_nodes("h4") %>% html_nodes("a") %>% html_text
case.info <- stri_trans_general(case.info, 'latin')


session <- html_session("https://sudact.ru")

i <- 1
for(i in 1:length(urls)){
  url.new <- paste0("https://sudact.ru", urls[i])
  session <- session %>% jump_to(url.new)

  decision <- session  %>% html_nodes("td.h-col1.h-col1-inner3") %>% html_text()
  decision.latin <- stri_trans_general(decision, 'latin')
  decision.latin <- gsub("\\{[^\\]]*\\}", "", decision.latin, perl=TRUE) #Removes text between curly braces
  decision.latin <- gsub("\\[[^\\]]*\\]", "", decision.latin, perl=TRUE) #Removes text between square brackets

  case.data.temp[,1] <- case.info[i]
  case.data.temp[,2] <- decision.latin
  case.data <- rbind(case.data, case.data.temp)
  print(paste("Item", i, "of", length(urls), "complete.", sep=" "))
 Sys.sleep(sample(10:39, 1))
  session <- session %>% back()
}
print(paste("Page", j, "complete. Switching to next page."))
}

write.csv(case.data, "article-142.1-cases.csv")
```



